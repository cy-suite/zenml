{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# Intro to MLOps using ZenML\n",
    "\n",
    "## üåç Overview\n",
    "\n",
    "This repository is a minimalistic MLOps project intended as a starting point to learn how to put ML workflows in production.\n",
    "\n",
    "Within this notebook we will show you how simple it is to switch from running code locally to running it remotely. You will then be able to explore all the metadata of your run in the ZenML Dashboard.\n",
    "\n",
    "<img src=\".assets/Overview.png\" width=\"50%\" alt=\"Quickstart Overview\">\n",
    "\n",
    "Follow along this notebook to understand how you can use ZenML to productionalize your ML workflows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f466b16",
   "metadata": {},
   "source": [
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to run this notebook, no local installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2977c",
   "metadata": {},
   "source": [
    "# üë∂ Step 0. Install Requirements\n",
    "\n",
    "Let's install ZenML and all requirement to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2f40eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zenml[server] in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (0.63.0)\n",
      "Requirement already satisfied: Jinja2 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (3.1.4)\n",
      "Requirement already satisfied: alembic<1.9.0,>=1.8.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (1.8.1)\n",
      "Requirement already satisfied: bcrypt==4.0.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (4.0.1)\n",
      "Requirement already satisfied: click<8.1.4,>=8.0.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (8.1.3)\n",
      "Requirement already satisfied: click-params<0.4.0,>=0.3.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.3.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.2.1)\n",
      "Requirement already satisfied: distro<2.0.0,>=1.6.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (1.9.0)\n",
      "Requirement already satisfied: docker<7.2.0,>=7.1.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (7.1.0)\n",
      "Requirement already satisfied: fastapi<=0.110,>=0.100 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.110.0)\n",
      "Requirement already satisfied: gitpython<4.0.0,>=3.1.18 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (3.1.43)\n",
      "Requirement already satisfied: httplib2<0.20,>=0.19.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.19.1)\n",
      "Requirement already satisfied: ipinfo>=4.4.3 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (5.0.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (1.26.4)\n",
      "Requirement already satisfied: orjson<3.11.0,>=3.10.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (3.10.6)\n",
      "Requirement already satisfied: packaging>=24.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (24.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.2.2)\n",
      "Requirement already satisfied: passlib<1.8.0,>=1.7.4 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from passlib[bcrypt]<1.8.0,>=1.7.4->zenml[server]) (1.7.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (6.0.0)\n",
      "Requirement already satisfied: pydantic<2.8,>=2.7 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.7.4)\n",
      "Requirement already satisfied: pydantic-settings<2.3.0,>=2.2.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.2.1)\n",
      "Requirement already satisfied: pyjwt==2.7.* in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pyjwt[crypto]==2.7.*; extra == \"server\"->zenml[server]) (2.7.0)\n",
      "Requirement already satisfied: pymysql<1.2.0,>=1.1.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.9.0.post0)\n",
      "Requirement already satisfied: python-multipart<0.1.0,>=0.0.9 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.0.9)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (6.0.2)\n",
      "Requirement already satisfied: rich>=12.0.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from rich[jupyter]>=12.0.0->zenml[server]) (13.7.1)\n",
      "Requirement already satisfied: secure<0.4.0,>=0.3.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.3.0)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (2.0.32)\n",
      "Requirement already satisfied: sqlalchemy_utils in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.41.2)\n",
      "Requirement already satisfied: sqlmodel==0.0.18 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from zenml[server]) (0.0.18)\n",
      "Requirement already satisfied: uvicorn>=0.17.5 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\"->zenml[server]) (0.30.5)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pyjwt[crypto]==2.7.*; extra == \"server\"->zenml[server]) (43.0.0)\n",
      "Requirement already satisfied: Mako in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from alembic<1.9.0,>=1.8.1->zenml[server]) (1.3.5)\n",
      "Requirement already satisfied: validators<0.19,>=0.18 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from click-params<0.4.0,>=0.3.0->zenml[server]) (0.18.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from docker<7.2.0,>=7.1.0->zenml[server]) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from docker<7.2.0,>=7.1.0->zenml[server]) (2.2.2)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from fastapi<=0.110,>=0.100->zenml[server]) (0.36.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from fastapi<=0.110,>=0.100->zenml[server]) (4.12.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from gitpython<4.0.0,>=3.1.18->zenml[server]) (4.0.11)\n",
      "Requirement already satisfied: cachetools in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipinfo>=4.4.3->zenml[server]) (5.4.0)\n",
      "Requirement already satisfied: aiohttp<=4 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipinfo>=4.4.3->zenml[server]) (3.10.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pandas>=1.1.5->zenml[server]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pandas>=1.1.5->zenml[server]) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pydantic<2.8,>=2.7->zenml[server]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pydantic<2.8,>=2.7->zenml[server]) (2.18.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pydantic-settings<2.3.0,>=2.2.1->zenml[server]) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.8.1->zenml[server]) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0->zenml[server]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0->zenml[server]) (2.18.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.5.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from rich[jupyter]>=12.0.0->zenml[server]) (8.1.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from sqlalchemy<3.0.0,>=2.0.0->zenml[server]) (3.0.3)\n",
      "Requirement already satisfied: h11>=0.8 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from uvicorn>=0.17.5->uvicorn[standard]>=0.17.5; extra == \"server\"->zenml[server]) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\"->zenml[server]) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\"->zenml[server]) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\"->zenml[server]) (0.23.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\"->zenml[server]) (12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from Jinja2->zenml[server]) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from aiohttp<=4->ipinfo>=4.4.3->zenml[server]) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from aiohttp<=4->ipinfo>=4.4.3->zenml[server]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from aiohttp<=4->ipinfo>=4.4.3->zenml[server]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from aiohttp<=4->ipinfo>=4.4.3->zenml[server]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from aiohttp<=4->ipinfo>=4.4.3->zenml[server]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from aiohttp<=4->ipinfo>=4.4.3->zenml[server]) (1.9.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from cryptography>=3.4.0->pyjwt[crypto]==2.7.*; extra == \"server\"->zenml[server]) (1.17.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.18->zenml[server]) (5.0.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (3.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->rich[jupyter]>=12.0.0->zenml[server]) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from requests>=2.26.0->docker<7.2.0,>=7.1.0->zenml[server]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from requests>=2.26.0->docker<7.2.0,>=7.1.0->zenml[server]) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from requests>=2.26.0->docker<7.2.0,>=7.1.0->zenml[server]) (2024.7.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from starlette<0.37.0,>=0.36.3->fastapi<=0.110,>=0.100->zenml[server]) (4.4.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from validators<0.19,>=0.18->click-params<0.4.0,>=0.3.0->zenml[server]) (5.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi<=0.110,>=0.100->zenml[server]) (1.3.1)\n",
      "Requirement already satisfied: pycparser in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]==2.7.*; extra == \"server\"->zenml[server]) (2.22)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (3.0.47)\n",
      "Requirement already satisfied: stack-data in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml[server]) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"zenml[server]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "# In case we are in a google colab, clone all additional relevant files\n",
    "if Environment.in_google_colab():\n",
    "    # Pull required modules from this example\n",
    "    !git clone -b main https://github.com/zenml-io/zenml\n",
    "    !cp -r zenml/examples/quickstart/* .\n",
    "    !rm -rf zenml\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76f562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart Kernel to ensure all libraries are properly loaded\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044374",
   "metadata": {},
   "source": [
    "\n",
    "Please wait for the installation to complete before running subsequent cells. At\n",
    "the end of the installation, the notebook kernel will restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce581",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 1: Connect to your ZenML Server\n",
    "To run this quickstart you need to connect to a ZenML Server. You can deploy it [yourself on your own infrastructure](https://docs.zenml.io/getting-started/deploying-zenml) or try it out for free, no credit-card required in our [ZenML Pro managed service](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2587315",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_server_url = \"INSERT_YOUR_SERVER_URL_HERE\"  # in the form \"https://URL_TO_SERVER\"\n",
    "\n",
    "!zenml connect --url $zenml_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ZenML and define the root for imports and docker builds\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48460",
   "metadata": {},
   "source": [
    "## ü•á Step 2: Build and run your first pipeline\n",
    "\n",
    "In this quickstart we'll be working with a small dataset of sentences in old english paired with more modern formulations. The task is a text-to-text transformation.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to break down your code into distinct functions that load your data, bring it into the correct shape and finally produce a model. This is the experimentation phase where we try to massage our data into the right format and feed it into our model training.\n",
    "\n",
    "<img src=\".assets/Experiment.png\" width=\"20%\" alt=\"Experimentation phase and pipeline construction\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import step\n",
    "from datasets import Dataset\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "@step\n",
    "def load_data() -> Annotated[Dataset, \"raw_dataset\"]:\n",
    "    \"\"\"Load and prepare the dataset.\"\"\"\n",
    "\n",
    "    def read_data(file_path):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                old, modern = line.strip().split(\"|\")\n",
    "                inputs.append(f\"Translate Old English to Modern English: {old}\")\n",
    "                targets.append(modern)\n",
    "\n",
    "        return {\"input\": inputs, \"target\": targets}\n",
    "\n",
    "    # Assuming your file is named 'translations.txt'\n",
    "    data = read_data(\"translations.txt\")\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6286b67",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines one step at a time.  If you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d838e2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"Translate Old English to Modern English: Shall I compare thee to a summer's day?\",\n",
       " 'target': 'Should I compare you to a summer day?'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_data()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05291",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the input/output pair looks to be in the right format ü•≥.\n",
    "\n",
    "For the sake of this quickstart we have prepared a few steps in the steps-directory. We'll now connect these together into a pipeline. To do this simply plug multiple steps together through their inputs and outputs. Then just add the `@pipeline` decorator to the function that connects the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50a9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mThe ZenML global configuration version (0.64.0) is higher than the version of ZenML currently being used (0.63.0). Read more about this issue and how to solve it here: \u001b[0m\u001b[1;36mhttps://docs.zenml.io/reference/global-settings#version-mismatch-downgrading\u001b[31m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from zenml import pipeline, Model\n",
    "from zenml.client import Client\n",
    "from zenml.model.model import Model\n",
    "\n",
    "from steps import load_data, tokenize_data, train_model, evaluate_model, model_tester\n",
    "from steps.model_trainer import T5_Model\n",
    "\n",
    "# Initialize the ZenML client to fetch objects from the ZenML Server\n",
    "client = Client()\n",
    "\n",
    "Client().activate_stack(\"default\") # We will start by using the default stack which is local\n",
    "\n",
    "model_name = \"YeOldeEnglishTranslator\"\n",
    "model = Model(\n",
    "  name = \"YeOldeEnglishTranslator\",\n",
    "  description = \"Model to translate from old to modern english\",\n",
    "  tags = [\"quickstart\", \"llm\", \"t5\"]\n",
    ")\n",
    "\n",
    "@pipeline(enable_cache=True, model=model)\n",
    "def english_translation_pipeline(\n",
    "    model_type: T5_Model,\n",
    "    per_device_train_batch_size: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "    dataloader_num_workers: int,\n",
    "    num_train_epochs: int = 5,\n",
    "):\n",
    "    \"\"\"Define a pipeline that connects the steps.\"\"\"\n",
    "    dataset, test_dataset = load_data()\n",
    "    tokenized_dataset, tokenizer = tokenize_data(dataset, model_type)\n",
    "    model = train_model(\n",
    "        tokenized_dataset,\n",
    "        model_type,\n",
    "        num_train_epochs,\n",
    "        per_device_train_batch_size,\n",
    "        gradient_accumulation_steps,\n",
    "        dataloader_num_workers,\n",
    "    )\n",
    "    evaluate_model(model, tokenized_dataset)\n",
    "    model_tester(model, tokenizer, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd73c23",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just as with the step - by calling the\n",
    "pipeline function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0aa9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36menglish_translation_pipeline\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mNew model version \u001b[0m\u001b[1;36m134\u001b[1;35m was created.\u001b[0m\n",
      "\u001b[1;35mDashboard URL for Model Version with name 134 : https://cloud.zenml.io/organizations/fc992c14-d960-4db7-812e-8f070c99c6f0/tenants/8a462fb6-b1fe-48df-9677-edc76bc8352d/model-versions/60713be3-f6af-48f2-9276-a886a44e4584\u001b[0m\n",
      "\u001b[1;35mExecuting a new run.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36malexej@zenml.io\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mDashboard URL: https://cloud.zenml.io/organizations/fc992c14-d960-4db7-812e-8f070c99c6f0/tenants/8a462fb6-b1fe-48df-9677-edc76bc8352d/runs/66a234a9-1348-47d1-b8fd-faa1c4c4c556\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mload_data\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mload_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtokenize_data\u001b[1;35m has started.\u001b[0m\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e236545bd5ce411eaf943c446b655ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc0ccb5effd48a4aa12a603be54b6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtokenize_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m7.900s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtokenize_data\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mCaching \u001b[0m\u001b[1;36mdisabled\u001b[1;35m explicitly for \u001b[0m\u001b[1;36mtrain_model\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_model\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 10:04, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.816400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_model\u001b[1;35m has finished in \u001b[0m\u001b[1;36m10m14s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_model\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has started.\u001b[0m\n",
      "Average loss on the dataset: 0.47016171691939235\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has finished in \u001b[0m\u001b[1;36m35.797s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_tester\u001b[1;35m has started.\u001b[0m\n",
      "/home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/alexej/.pyenv/versions/3.11.9/envs/clean4/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "\u001b[1;35mGenerated Old English: Forsooth, I say unto thee\u001b[0m\n",
      "\u001b[1;35mModel Translation:  \n",
      "\u001b[0m\n",
      "\u001b[1;35mGenerated Old English: Hark! What light through yonder window breaks?\u001b[0m\n",
      "\u001b[1;35mModel Translation: Warum st√º√ºnnt die Fenster? \n",
      "\u001b[0m\n",
      "\u001b[1;35mGenerated Old English: Wherefore art thou Romeo?\u001b[0m\n",
      "\u001b[1;35mModel Translation:  \n",
      "\u001b[0m\n",
      "\u001b[1;35mGenerated Old English: A horse! A horse! My kingdom for a horse!\u001b[0m\n",
      "\u001b[1;35mModel Translation: (, oder Un: Das- Die Ein Les \n",
      "\u001b[0m\n",
      "\u001b[1;35mGenerated Old English: Out, damned spot! Out, I say!\u001b[0m\n",
      "\u001b[1;35mModel Translation: , () Un; \n",
      "\u001b[0m\n",
      "\u001b[1;35mGenerated Old English: Friends, Romans, countrymen, lend me your ears\u001b[0m\n",
      "\u001b[1;35mModel Translation:  \n",
      "\u001b[0m\n",
      "\u001b[1;35mGenerated Old English: √∂√∂√º√º√§√§\u001b[0m\n",
      "\u001b[1;35mModel Translation: √∂√∂√º√º√§√§√º√∂√§√∂√º√§√∂ √ºu√º√©√§ \n",
      "\u001b[0m\n",
      "{'Prompt 0': {'Forsooth, I say unto thee': ''}, 'Prompt 1': {'Hark! What light through yonder window breaks?': 'Warum st√º√ºnnt die Fenster?'}, 'Prompt 2': {'Wherefore art thou Romeo?': ''}, 'Prompt 3': {'A horse! A horse! My kingdom for a horse!': '(, oder Un: Das- Die Ein Les'}, 'Prompt 4': {'Out, damned spot! Out, I say!': ', () Un;'}, 'Prompt 5': {'Friends, Romans, countrymen, lend me your ears': ''}, 'Prompt 6': {'√∂√∂√º√º√§√§': '√∂√∂√º√º√§√§√º√∂√§√∂√º√§√∂ √ºu√º√©√§'}}\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_tester\u001b[1;35m has finished in \u001b[0m\u001b[1;36m4.970s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_tester\u001b[1;35m completed successfully.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m11m13s\u001b[1;35m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline and configure some parameters at runtime\n",
    "pipeline_run = english_translation_pipeline(\n",
    "    model_type=\"t5-small\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42078a",
   "metadata": {},
   "source": [
    "As you can see the pipeline has run successfully. It also printed out some examples - however it seems the model is not yet able to solve the task well. But we validated that the pipeline works.\n",
    "\n",
    "<img src=\".assets/DAG.png\" width=\"50%\" alt=\"Dashboard view\">\n",
    "\n",
    "Above you can see what the dashboard view of the pipeline in the ZenML Dashboard. You can find the URL for this in the logs above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037f09d",
   "metadata": {},
   "source": [
    "We can now access the trained model and it's tokenizer from the ZenML Model Control Plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bceb0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model object\n",
    "model = client.get_model_version(model_name).get_model_artifact('model').load()\n",
    "tokenizer = client.get_model_version(model_name).get_artifact('tokenizer').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd8592-2295-4421-a6e6-f619ed389e8c",
   "metadata": {},
   "source": [
    "With this in hand we can now play around with the model directly and try out some examples ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e514ac-1a0a-49a0-b8a4-e33cee12c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_text = \"I do desire we may be better strangers\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e653c7a-4073-424e-8a59-c69f49526b96",
   "metadata": {},
   "source": [
    "## Lets recap what we've done so far\n",
    "\n",
    "We created a modular pipeline, this pipeline is modularly constructed from different steps. We have shown that this pipeline runs locally.\n",
    "\n",
    "As expected, the modcel does not yet solve its task. To train a model that can solve our task well, we would have to train a larger model for longer. For this, we'll need to move away from our local environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# ‚åö Step 3: Scale it up in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791b32b-f6be-4ae2-867c-5e628f363858",
   "metadata": {},
   "source": [
    "Our last section confirmed to us, that the pipeline works. Let's now run the pipeline in the environment of your choice.\n",
    "\n",
    "For you to be able to try this step, you will need to have access to a cloud environment (AWS, GCP, AZURE). ZenML wraps around all the major cloud providers and orchestration tools and lets you easily deploy your code onto them.\n",
    "\n",
    "To do this lets head over to the `Stack` section of your ZenML Dashboard. Here you'll be able to either connect to an existing or deploy a new environment. Choose on of the options presented to you there and come back when you have a stack ready to go. Then proceed to the appropirate section below. **Do not** run all three. Also be sure that you are running with a remote ZenML server (see Step 1 above).\n",
    "\n",
    "<img src=\".assets/StackCreate.png\" width=\"20%\" alt=\"Stack creation in the ZenML Dashboard\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e02652-34ae-4b79-948e-1d80f559fdf5",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a95e2a-2c55-4068-8111-5ea5559203da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install gcp -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "from zenml.integrations.gcp.flavors.vertex_orchestrator_flavor import VertexOrchestratorSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa58b33-712c-4926-b12b-feceda3384c8",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95af245-a7fd-4d64-b0af-d5a96a788846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker s3 s3fs\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "from zenml.integrations.aws.flavors.sagemaker_orchestrator_flavor import SagemakerOrchestratorSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"alexej-aws-quickstart-stack\" #\"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\"),\n",
    "        \"orchestrator.sagemaker\": SagemakerOrchestratorSettings(instance_type=\"ml.p2.xlarge\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07454193-8bc8-4adf-bcca-db598b891ccf",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63c109-0ba5-4a62-adaa-47aa9612373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install azure -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "from zenml.integrations.skypilot.flavors.skypilot_orchestrator_base_vm_config import SkypilotBaseOrchestratorSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"orchestrator.sagemaker\": SkypilotBaseOrchestratorSettings(accelerators='V100', memory=\"32+\", cpus=\"8+\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f17b7a-5a82-4975-b9bd-6a63fbb97a68",
   "metadata": {},
   "source": [
    "## üöÄ Ready to launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f30c-9a8e-46ca-ba44-cf16ea715dac",
   "metadata": {},
   "source": [
    "We now have configured zenml to use your very own cloud infrastructure.\n",
    "\n",
    "<img src=\".assets/SwitchStack.png\" width=\"20%\" alt=\"Stack switching with ZenML\">\n",
    "\n",
    "For the next pipeline run, we'll be training the same t5 model (`t5_small`) on your own infrastrucutre.\n",
    "\n",
    "Note: The whole process may take a bit longer the first time around, as your pipeline code needs to be built into docker containers to be run in the orchestration environment of your stack. Any consecutive run of the pipeline, even with different parameters set, will not take as long again thanks to docker caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e758fe-6ea3-42ff-bea8-33953135bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = configured_english_translation_pipeline(\n",
    "    model_type=\"t5-small\", \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eef2c6-6dfb-4b67-9883-594a0df20173",
   "metadata": {},
   "source": [
    "You did it! You build a pipeline locally, verified that all its parts work well together and now are running it on a production environment\n",
    "\n",
    "<img src=\".assets/Production.png\" width=\"20%\" alt=\"Pipeline running on your infrastructure.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231677c-1fd6-4ec3-8c8c-47fd9406072e",
   "metadata": {},
   "source": [
    "## Now its Up to you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c31e0d-dfab-4692-a406-0dc439f25443",
   "metadata": {},
   "source": [
    "You can now start worrying about making the model actually work well, as the model results are still not acceptable.\n",
    "\n",
    "Here are some things that you could do:\n",
    "\n",
    "* Iterate on the training data and its tokenization\n",
    "* You can switch out the model itself. Instead of `model_type=\"t5_small\"` you could use `model_type=\"t5_large\"` for example\n",
    "* You can train for longer by increasing the `num_train_epochs=xxx`. In order to speed this up you can also add accelerators to your orchestrators. Learn more about this in the section below.\n",
    "\n",
    "No matter what avenue you choose to actually make the model work, we would love to see how you did it, so please reach out and share your solution with us either on [**Slack Community**](https://zenml.io/slack) or through our email hello@zenml.io."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03054e-8b3e-4edb-9d87-82ae51693d2d",
   "metadata": {},
   "source": [
    "## Adding Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec09c1c-4f6e-4f59-a99e-87500004a174",
   "metadata": {},
   "source": [
    "Each of the cloud providers allows users to add accelerators to their serverless offerings. Here's what you need to add to the pipeline settings in order to unlock gpus. Keep in mind, that you might have to increase your quotas within the cloud providers.\n",
    "\n",
    "### GCP\n",
    "\n",
    "```\n",
    "english_translation_pipeline.with_options(settings="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "\n",
    "This was just the tip of the iceberg of what ZenML can do; check out the [**docs**](https://docs.zenml.io/) to learn more\n",
    "about the capabilities of ZenML. For example, you might want to:\n",
    "\n",
    "- [Deploy ZenML](https://docs.zenml.io/user-guide/production-guide/connect-deployed-zenml) to collaborate with your colleagues.\n",
    "- Run the same pipeline on a [cloud MLOps stack in production](https://docs.zenml.io/user-guide/production-guide/cloud-stack).\n",
    "- Track your metrics in an experiment tracker like [MLflow](https://docs.zenml.io/stacks-and-components/component-guide/experiment-trackers/mlflow).\n",
    "\n",
    "## What next?\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack) and become part of the ZenML family!\n",
    "* If you want to quickly get started with ZenML, check out [ZenML Pro](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560354d-9e78-4061-aaff-2e6213229911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
