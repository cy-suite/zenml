{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# Intro to MLOps using ZenML\n",
    "\n",
    "## üåç Overview\n",
    "\n",
    "This repository is a minimalistic MLOps project intended as a starting point to learn how to put ML workflows in production. It features: \n",
    "\n",
    "- A very simple training pipeline that loads the a dataset and trains a model.\n",
    "\n",
    "Within this notebook we will show you how simple it is to switch where your code runs and where your data is stored. You will also learn how all the metadata of your run is stored and accessible through ZenML.\n",
    "\n",
    "Follow along this notebook to understand how you can use ZenML to productionalize your ML workflows!\n",
    "\n",
    "<img src=\".assets/pipeline_overview.png\" width=\"50%\" alt=\"Pipelines Overview\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f466b16",
   "metadata": {},
   "source": [
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to see ZenML in action, no installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2977c",
   "metadata": {},
   "source": [
    "# üë∂ Step 0. Install Requirements\n",
    "\n",
    "Let's install ZenML to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"zenml[server]\" pyarrow\n",
    "!zenml integration install sklearn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "# In case we are in a google colab, clone all additional relevant files\n",
    "if Environment.in_google_colab():\n",
    "    # Pull required modules from this example\n",
    "    !git clone -b main https://github.com/zenml-io/zenml\n",
    "    !cp -r zenml/examples/quickstart/* .\n",
    "    !rm -rf zenml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Kernel to ensure all libraries are properly loaded\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044374",
   "metadata": {},
   "source": [
    "\n",
    "Please wait for the installation to complete before running subsequent cells. At\n",
    "the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce581",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 1: Connect to your ZenML Server\n",
    "To run this quickstart you need to connect to a ZenML Server. You can deploy it [yourself](https://docs.zenml.io/getting-started/deploying-zenml) or try it out for free, no credit-card required in our [ZenML Pro managed service](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2587315",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_server_url = \"https://1cf18d95-zenml.cloudinfra.zenml.io\"  # in the form \"https://URL_TO_SERVER\"\n",
    "\n",
    "!zenml connect --url $zenml_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ZenML and set the default stack\n",
    "!zenml init\n",
    "\n",
    "!zenml stack set default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6b4c8-09a1-48ba-b971-662cba06745b",
   "metadata": {},
   "source": [
    "Default stack in this case means the code will run on the machine that is running this notebook and all output data will be stored there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f775f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the imports at the top\n",
    "from typing_extensions import Annotated\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from zenml import step, pipeline, Model, get_step_context\n",
    "from zenml.client import Client\n",
    "from zenml.logger import get_logger\n",
    "from uuid import UUID\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "from zenml import pipeline\n",
    "\n",
    "from steps import (\n",
    "    data_loader,\n",
    "    data_preprocessor,\n",
    "    data_splitter,\n",
    "    model_evaluator,\n",
    "    inference_preprocessor,\n",
    "    model_trainer,\n",
    "    model_evaluator\n",
    ")\n",
    "\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize the ZenML client to fetch objects from the ZenML Server\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48460",
   "metadata": {},
   "source": [
    "## ü•á Step 2: Run your first pipeline\n",
    "\n",
    "We'll start off by importing our data and training a simple ml model. In this quickstart we'll be working with\n",
    "[the Breast Cancer](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) dataset\n",
    "which is publicly available on the UCI Machine Learning Repository. The task is a classification\n",
    "problem, to predict whether a patient is diagnosed with breast cancer or not.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to break down your code into distinct functions that load your data, bring it into the correct shape and finally produce a model. H#ere is our first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def data_loader_simplified(\n",
    "    random_state: int, is_inference: bool = False, target: str = \"target\"\n",
    ") -> Annotated[pd.DataFrame, \"dataset\"]:  # We name the dataset \n",
    "    \"\"\"Dataset reader step.\"\"\"\n",
    "    dataset = load_breast_cancer(as_frame=True)\n",
    "    inference_size = int(len(dataset.target) * 0.05)\n",
    "    dataset: pd.DataFrame = dataset.frame\n",
    "    inference_subset = dataset.sample(inference_size, random_state=random_state)\n",
    "    if is_inference:\n",
    "        dataset = inference_subset\n",
    "        dataset.drop(columns=target, inplace=True)\n",
    "    else:\n",
    "        dataset.drop(inference_subset.index, inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    logger.info(f\"Dataset with {len(dataset)} records loaded!\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba4c6",
   "metadata": {},
   "source": [
    "The whole function is decorated with the ZenML-`@step` decorator. Once this step is added to a pipeline, ZenML will automatically version, track, and cache the data that is produced by this function as an `artifact`. This enables you to \n",
    "reproduce your data at any point in the future, even if the original data source\n",
    "changes or disappears. \n",
    "\n",
    "Note the typing of the function outputs. These are not only good practice, but also\n",
    "help ZenML store and load your data appropriately. ABy using `Annotated` type hint in the output of the\n",
    "step, we are also naming our outputs. This will make\n",
    "it possible to access it by name later on.\n",
    "\n",
    "You'll also notice that we have included type hints for the outputs\n",
    "to the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6286b67",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines one step at a time.  If you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_loader_simplified(random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05291",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the values are all in the right format ü•≥.\n",
    "\n",
    "We're now at the point where can bring this step (and some others) together into a single\n",
    "pipeline. To do this simply plug multiple steps together through their inputs and outputs.\n",
    "Then just add the `@pipeline` decorator to the function that connects the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(model=Model(name=\"demo\", description=\"Show case Model Control Plane.\"), enable_cache=False)\n",
    "def training_pipeline(\n",
    "    test_size: float = 0.3,\n",
    "    drop_na: Optional[bool] = None,\n",
    "    normalize: Optional[bool] = None,\n",
    "    drop_columns: Optional[List[str]] = None,\n",
    "    target: Optional[str] = \"target\",\n",
    "    random_state: int = 17,\n",
    "    model_type: Optional[str] = \"sgd\"\n",
    "):\n",
    "    \"\"\"Feature engineering pipeline.\"\"\"\n",
    "    # Link all the steps together by calling them and passing the output\n",
    "    # of one step as the input of the next step.\n",
    "    raw_data = data_loader(random_state=random_state, target=target)\n",
    "    dataset_trn, dataset_tst = data_splitter(\n",
    "        dataset=raw_data,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "    dataset_trn, dataset_tst, _ = data_preprocessor(\n",
    "        dataset_trn=dataset_trn,\n",
    "        dataset_tst=dataset_tst,\n",
    "        drop_na=drop_na,\n",
    "        normalize=normalize,\n",
    "        drop_columns=drop_columns,\n",
    "        target=target,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    trained_model = model_trainer(\n",
    "        dataset_trn=dataset_trn,\n",
    "        model_type=model_type,\n",
    "    )\n",
    "\n",
    "    acc = model_evaluator(\n",
    "        model=trained_model,\n",
    "        dataset_trn=dataset_trn,\n",
    "        dataset_tst=dataset_tst,\n",
    "        target=target,\n",
    "    )\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd73c23",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just as with the step - by calling the\n",
    "pipeline function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = training_pipeline(model_type=\"sgd\")  # model_type=\"rf\" is also supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42078a",
   "metadata": {},
   "source": [
    "As you can see the pipeline has run succesfully. Lets check this out by following the Dashboard URL that you can find in the logs above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8471f93",
   "metadata": {},
   "source": [
    "We can also fetch the pipeline from the server and view the results directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "run = client.get_pipeline(\"training_pipeline\").last_run\n",
    "print(run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037f09d",
   "metadata": {},
   "source": [
    "We can also see the data artifacts that were produced by the last step of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34283e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.steps[\"data_preprocessor\"].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one of the training datasets\n",
    "run.steps[\"data_preprocessor\"].outputs[\"dataset_trn\"].load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# ‚åö Step 3: Run the same pipeline on your cloud"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fbe678d-efaa-4263-a148-b97bf4e91481",
   "metadata": {},
   "source": [
    "Now that we have run our pipeline in our development environment, lets take this to the next level and run the exact same code in the environment of your choice. ZenML calls these environment Stacks. A stack consists of different components, however you'll only really need to care about the compute and data storage for this example.\n",
    "\n",
    "For you to be able tp try thsi step you will need to have acess to some cloud compute and cloud storage somewhere (aws, gcp, azure, etc...). ZenML wrapps around all the major cloud providers and orchestration tools and lets you easily plug your code into them.\n",
    "\n",
    "To do this lets head over to the Stack section of your ZenML Dashboard. Here you'll be able to either connect to an existing or deploy a new environment. Choose on of the options presented to you there and come back when you have a stack ready to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa9eec-c679-476a-a49a-9ce9f3aef37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f30c-9a8e-46ca-ba44-cf16ea715dac",
   "metadata": {},
   "source": [
    "We now have set zenml to use this stack for the next pipeline run, lets see this in action by rerunning our previous pipeline.\n",
    "\n",
    "Note: This may take a bit longer the first time around, as your pipeline code needs to be built into docker containers to be run in the orchestration environment of your stack. Any consecutive run of the pipeline, even with different parameters set, will not take as long again thanks to docker caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e758fe-6ea3-42ff-bea8-33953135bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = training_pipeline(model_type=\"sgd\")  # model_type=\"rf\" is also supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6397a-9483-4b3e-8661-b78efaecf947",
   "metadata": {},
   "source": [
    "As you can see, you successfully ran your pipeline in a production ready environment with access to GPUs, scalable compute and large data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You're a legit MLOps engineer now! You have created a training pipeline and you\n",
    "have deployed it into a production-ready environment with the compute of your \n",
    "choice. You also have gotten a hang of the ZenML Dashboard.\n",
    "\n",
    "## Further exploration\n",
    "\n",
    "This was just the tip of the iceberg of what ZenML can do; check out the [**docs**](https://docs.zenml.io/) to learn more\n",
    "about the capabilities of ZenML. For example, you might want to:\n",
    "\n",
    "- [Deploy ZenML](https://docs.zenml.io/user-guide/production-guide/connect-deployed-zenml) to collaborate with your colleagues.\n",
    "- Run the same pipeline on a [cloud MLOps stack in production](https://docs.zenml.io/user-guide/production-guide/cloud-stack).\n",
    "- Track your metrics in an experiment tracker like [MLflow](https://docs.zenml.io/stacks-and-components/component-guide/experiment-trackers/mlflow).\n",
    "\n",
    "## What next?\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack) and become part of the ZenML family!\n",
    "* If you want to quickly get started with ZenML, check out [ZenML Pro](https://zenml.io/pro)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
