{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# Intro to MLOps using ZenML\n",
    "\n",
    "## üåç Overview\n",
    "\n",
    "This repository is a minimalistic MLOps project intended as a starting point to learn how to put ML workflows in production.\n",
    "\n",
    "Within this notebook we will show you how simple it is to switch from running code locally to running it remotely. You will then be able to explore all the metadata of your run in the ZenML Dashboard.\n",
    "\n",
    "<img src=\".assets/Overview.png\" width=\"50%\" alt=\"Quickstart Overview\">\n",
    "\n",
    "Follow along this notebook to understand how you can use ZenML to productionalize your ML workflows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f466b16",
   "metadata": {},
   "source": [
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to run this notebook, no local installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2977c",
   "metadata": {},
   "source": [
    "# üë∂ Step 0. Install Requirements\n",
    "\n",
    "Let's install ZenML and all requirement to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"zenml[server]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "# In case we are in a google colab, clone all additional relevant files\n",
    "if Environment.in_google_colab():\n",
    "    # Pull required modules from this example\n",
    "    !git clone -b main https://github.com/zenml-io/zenml\n",
    "    !cp -r zenml/examples/quickstart/* .\n",
    "    !rm -rf zenml\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Kernel to ensure all libraries are properly loaded\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044374",
   "metadata": {},
   "source": [
    "\n",
    "Please wait for the installation to complete before running subsequent cells. At\n",
    "the end of the installation, the notebook kernel will restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce581",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 1: Connect to your ZenML Server\n",
    "To run this quickstart you need to connect to a ZenML Server. You can deploy it [yourself on your own infrastructure](https://docs.zenml.io/getting-started/deploying-zenml) or try it out for free, no credit-card required in our [ZenML Pro managed service](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2587315",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_server_url = \"INSERT_YOUR_SERVER_URL_HERE\"  # in the form \"https://URL_TO_SERVER\"\n",
    "\n",
    "!zenml connect --url $zenml_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ZenML and define the root for imports and docker builds\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48460",
   "metadata": {},
   "source": [
    "## ü•á Step 2: Build and run your first pipeline\n",
    "\n",
    "In this quickstart we'll be working with a small dataset of sentences in old english paired with more modern formulations. The task is a text-to-text transformation.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to break down your code into distinct functions that load your data, bring it into the correct shape and finally produce a model. This is the experimentation phase where we try to massage our data into the right format and feed it into our model training.\n",
    "\n",
    "<img src=\".assets/Experiment.png\" height=\"30%\" alt=\"Experimentation phase and pipeline construction\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import step\n",
    "from datasets import Dataset\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "@step\n",
    "def load_data() -> Annotated[Dataset, \"raw_dataset\"]:\n",
    "    \"\"\"Load and prepare the dataset.\"\"\"\n",
    "\n",
    "    def read_data(file_path):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                old, modern = line.strip().split(\"|\")\n",
    "                inputs.append(f\"Translate Old English to Modern English: {old}\")\n",
    "                targets.append(modern)\n",
    "\n",
    "        return {\"input\": inputs, \"target\": targets}\n",
    "\n",
    "    # Assuming your file is named 'translations.txt'\n",
    "    data = read_data(\"translations.txt\")\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6286b67",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines one step at a time.  If you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05291",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the input/output pair looks to be in the right format ü•≥.\n",
    "\n",
    "For the sake of this quickstart we have prepared a few steps in the steps-directory. We'll now connect these together into a pipeline. To do this simply plug multiple steps together through their inputs and outputs. Then just add the `@pipeline` decorator to the function that connects the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline, Model\n",
    "from zenml.client import Client\n",
    "from zenml.model.model import Model\n",
    "\n",
    "from steps import load_data, tokenize_data, train_model, evaluate_model, test_random_sentences\n",
    "from steps.model_trainer import T5_Model\n",
    "\n",
    "# Initialize the ZenML client to fetch objects from the ZenML Server\n",
    "client = Client()\n",
    "\n",
    "Client().activate_stack(\"default\") # We will start by using the default stack which is local\n",
    "\n",
    "model_name = \"YeOldeEnglishTranslator\"\n",
    "model = Model(\n",
    "  name = \"YeOldeEnglishTranslator\",\n",
    "  description = \"Model to translate from old to modern english\",\n",
    "  tags = [\"quickstart\", \"llm\", \"t5\"]\n",
    ")\n",
    "\n",
    "@pipeline(enable_cache=True, model=model)\n",
    "def english_translation_pipeline(\n",
    "    model_type: T5_Model,\n",
    "    num_train_epochs: int,\n",
    "    per_device_train_batch_size: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "    dataloader_num_workers: int,\n",
    "):\n",
    "    \"\"\"Define a pipeline that connects the steps.\"\"\"\n",
    "    dataset = load_data()\n",
    "    tokenized_dataset = tokenize_data(dataset)\n",
    "    model, tokenizer = train_model(\n",
    "        tokenized_dataset,\n",
    "        model_type,\n",
    "        num_train_epochs,\n",
    "        per_device_train_batch_size,\n",
    "        gradient_accumulation_steps,\n",
    "        dataloader_num_workers,\n",
    "    )\n",
    "    evaluate_model(model, tokenized_dataset)\n",
    "    test_random_sentences(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd73c23",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just as with the step - by calling the\n",
    "pipeline function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline and configure some parameters at runtime\n",
    "pipeline_run = english_translation_pipeline(\n",
    "    model_type=\"t5-small\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42078a",
   "metadata": {},
   "source": [
    "As you can see the pipeline has run succesfully. It also printed out some examples - however it seems the model is not yet able to solve the task well. But we validated that the pipeline works.\n",
    "\n",
    "<img src=\".assets/DAG.png\" width=\"50%\" alt=\"Dashboard view\">\n",
    "\n",
    "Above you can see what the dahboard view of the pipeline in the ZenML Dashboard. You can find the URL for this in the logs above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037f09d",
   "metadata": {},
   "source": [
    "We can now access the trained model and it's tokenizer from the ZenML Model Control Plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model object\n",
    "model = client.get_model_version(model_name).get_model_artifact('model').load()\n",
    "tokenizer = client.get_model_version(model_name).get_artifact('tokenizer').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd8592-2295-4421-a6e6-f619ed389e8c",
   "metadata": {},
   "source": [
    "With this in hand we can now play around with the model directly and try out some examples ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e514ac-1a0a-49a0-b8a4-e33cee12c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test_text = \"I do desire we may be better strangers\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e653c7a-4073-424e-8a59-c69f49526b96",
   "metadata": {},
   "source": [
    "## Lets recap what we've done so far\n",
    "\n",
    "We created a modular pipeline, this pipeline is modularly constructed from different steps. We have shown that this pipeline runs locally.\n",
    "\n",
    "As expected, the modcel does not yet solve its task. To train a model that can solve our task well, we would have to train a larger model for longer. For this, we'll need to move away from our local environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# ‚åö Step 3: Scale it up in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791b32b-f6be-4ae2-867c-5e628f363858",
   "metadata": {},
   "source": [
    "Our last section confirmed to us, that the pipeline works. Let's now run the pipeline in the environment of your choice.\n",
    "\n",
    "For you to be able to try this step, you will need to have acess to a cloud environment (AWS, GCP, AZURE). ZenML wrapps around all the major cloud providers and orchestration tools and lets you easily deploy your code onto them.\n",
    "\n",
    "To do this lets head over to the `Stack` section of your ZenML Dashboard. Here you'll be able to either connect to an existing or deploy a new environment. Choose on of the options presented to you there and come back when you have a stack ready to go. Then proceed to the appropirate section below. **Do not** run all three. Also be sure that you are running with a remote ZenML server (see Step 1 above).\n",
    "\n",
    "<img src=\".assets/StackCreate.png\" height=\"30%\" alt=\"Stack creation in the ZenML Dashboard\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e02652-34ae-4b79-948e-1d80f559fdf5",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a95e2a-2c55-4068-8111-5ea5559203da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install gcp -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "from zenml.integrations.gcp.flavors.vertex_orchestrator_flavor import VertexOrchestratorSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\"),\n",
    "        \"orchestrator.vertex\": VertexOrchestratorSettings(node_selector_constraint=(\"cloud.google.com/gke-accelerator\", \"NVIDIA_TESLA_P4\"))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa58b33-712c-4926-b12b-feceda3384c8",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95af245-a7fd-4d64-b0af-d5a96a788846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install aws s3 -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "from zenml.integrations.aws.flavors.sagemaker_orchestrator_flavor import SagemakerOrchestratorSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\"),\n",
    "        \"orchestrator.sagemaker\": SagemakerOrchestratorSettings(instance_type=\"ml.p2.xlarge\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07454193-8bc8-4adf-bcca-db598b891ccf",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63c109-0ba5-4a62-adaa-47aa9612373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install azure -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "from zenml.integrations.skypilot.flavors.skypilot_orchestrator_base_vm_config import SkypilotBaseOrchestratorSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"orchestrator.sagemaker\": SkypilotBaseOrchestratorSettings(accelerators='V100', memory=\"32+\", cpus=\"8+\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f17b7a-5a82-4975-b9bd-6a63fbb97a68",
   "metadata": {},
   "source": [
    "## üöÄ Ready to launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f30c-9a8e-46ca-ba44-cf16ea715dac",
   "metadata": {},
   "source": [
    "We now have configured zenml to use your very own cloud infrastructure.\n",
    "\n",
    "<img src=\".assets/SwitchStack.png\" height=\"30%\" alt=\"Stack switching with ZenML\">\n",
    "\n",
    "For the next pipeline run, we'll be training the same t5 model (`t5_small`) on your own infrastrucutre.\n",
    "\n",
    "Note: The whole process may take a bit longer the first time around, as your pipeline code needs to be built into docker containers to be run in the orchestration environment of your stack. Any consecutive run of the pipeline, even with different parameters set, will not take as long again thanks to docker caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e758fe-6ea3-42ff-bea8-33953135bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = configured_english_translation_pipeline(\n",
    "    model_type=\"t5-small\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eef2c6-6dfb-4b67-9883-594a0df20173",
   "metadata": {},
   "source": [
    "You did it!\n",
    "\n",
    "<img src=\".assets/Production.png\" width=\"50%\" alt=\"Pipeline running on your infrastructure.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231677c-1fd6-4ec3-8c8c-47fd9406072e",
   "metadata": {},
   "source": [
    "## Now its Up to you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c31e0d-dfab-4692-a406-0dc439f25443",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You're a legit MLOps engineer now! You have created a training pipeline and you\n",
    "have deployed it into a production-ready environment with the compute of your \n",
    "choice. You also have gotten a hang of the ZenML Dashboard.\n",
    "\n",
    "## Further exploration\n",
    "\n",
    "This was just the tip of the iceberg of what ZenML can do; check out the [**docs**](https://docs.zenml.io/) to learn more\n",
    "about the capabilities of ZenML. For example, you might want to:\n",
    "\n",
    "- [Deploy ZenML](https://docs.zenml.io/user-guide/production-guide/connect-deployed-zenml) to collaborate with your colleagues.\n",
    "- Run the same pipeline on a [cloud MLOps stack in production](https://docs.zenml.io/user-guide/production-guide/cloud-stack).\n",
    "- Track your metrics in an experiment tracker like [MLflow](https://docs.zenml.io/stacks-and-components/component-guide/experiment-trackers/mlflow).\n",
    "\n",
    "## What next?\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack) and become part of the ZenML family!\n",
    "* If you want to quickly get started with ZenML, check out [ZenML Pro](https://zenml.io/pro)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
